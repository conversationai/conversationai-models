{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from IPython.display import display\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read scored test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_data_path = 'gs://conversationai-models/biosbias/scored_data/test_standard_0409.csv'\n",
    "scrubbed_data_path = 'gs://conversationai-models/biosbias/scored_data/test_very_scrubbed_0409.csv'\n",
    "\n",
    "perf_df = pd.read_csv(tf.gfile.Open(standard_data_path)).drop_duplicates(subset=['tokens'])\n",
    "scrubbed_df = pd.read_csv(tf.gfile.Open(scrubbed_data_path)).drop_duplicates(subset=['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perf_df.shape)\n",
    "print(scrubbed_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = perf_df.join(scrubbed_df, rsuffix = '_scrubbed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_col_name(col_name):\n",
    "    #print(col_name)\n",
    "    pattern = r'^.*_(\\d+)$'\n",
    "    return int(re.search(pattern, col_name).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_class(df, model_name, class_names):\n",
    "    model_class_names = ['{}_{}'.format(model_name, class_name) for class_name in class_names]\n",
    "    sub_df = df[model_class_names]\n",
    "    df['{}_class'.format(model_name)] = sub_df.idxmax(axis=1).apply(get_class_from_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = {\n",
    "    'tf_trainer_tf_gru_attention_multiclass_biosbias_glove:v_20190328_103117': 'glove',\n",
    "    'tf_trainer_tf_gru_attention_multiclass_biosbias_glove:v_20190410_174837': 'debiased_tolga',\n",
    "    'tf_trainer_tf_gru_attention_multiclass_biosbias_glove:v_20190410_174941': 'debiased_biosbias',\n",
    "    'tf_trainer_tf_gru_attention_multiclass_biosbias_glove:v_20190410_175003': 'strongdebias_1',\n",
    "    'tf_trainer_tf_gru_attention_multiclass_biosbias_glove:v_20190410_175019': 'strongdebias_2',\n",
    "    'tf_trainer_tf_gru_attention_multiclass_biosbias_glove:v_20190410_175034': 'strongdebias_3',\n",
    "    'tf_trainer_tf_gru_attention_multiclass_biosbias_glove:v_20190410_175055': 'strongdebias_4', \n",
    "    'tf_trainer_tf_gru_attention_multiclass_biosbias_glove:v_20190410_175113': 'no_equalize',\n",
    "    'tf_trainer_tf_gru_attention_multiclass_biosbias_glove:v_20190410_175131': 'no_proj',\n",
    "    'tf_trainer_tf_gru_attention_multiclass_biosbias_glove:v_20190410_175254': 'very_scrubbed'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = range(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _model in MODEL_NAMES:\n",
    "    find_best_class(df, _model, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels with either gender having too few examples\n",
    "bad_labels = df.groupby('label').gender.value_counts().reset_index(name = 'count').query('count < 4').label.values\n",
    "assert len(bad_labels) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "for _model in MODEL_NAMES:\n",
    "    is_correct = (df['{}_class'.format(_model)] == df['label'])\n",
    "    _acc = sum(is_correct)/len(is_correct)\n",
    "    accuracy_list.append(_acc)\n",
    "    print ('Accuracy for model {}: {}'.format(MODEL_NAMES[_model], _acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _class in CLASS_NAMES:\n",
    "    df['label_{}'.format(_class)] = (df['label'] == _class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender ratios of classes\n",
    "gender_counts = df.groupby('label').gender.value_counts().reset_index(name = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frac_female(df):\n",
    "    m_count = df[df['gender'] == \"M\"]['count'].values[0]\n",
    "    f_count = df[df['gender'] == \"F\"]['count'].values[0]\n",
    "    return {'label': df['label'].values[0], 'frac_female': f_count/(m_count+f_count)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_female_df = pd.DataFrame(list(gender_counts.groupby('label', as_index = False).apply(frac_female)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITH THRESHOLD\n",
    "\n",
    "# def compute_tpr(df, _class, _model, threshold = 0.5):\n",
    "#     tpr = metrics.recall_score(df['label_{}'.format(_class)],\n",
    "#                                df['{}_{}'.format(_model,_class)] > threshold)\n",
    "#     return tpr\n",
    "    \n",
    "# def compute_tpr_tnr(df, _class, _model, threshold = 0.5):\n",
    "#     #cm = metrics.confusion_matrix(df['label_{}'.format(_class)],\n",
    "#     #                              df['{}_{}'.format(_model,_class)] > threshold)\n",
    "#     cm = pd.crosstab(df['label_{}'.format(_class)], df['{}_{}'.format(_model,_class)] > threshold)\n",
    "#     #display(cm)\n",
    "#     if cm.shape[0] > 1:\n",
    "#         tn = cm.iloc[0,0]\n",
    "#         fp = cm.iloc[0,1]\n",
    "#         fn = cm.iloc[1,0]\n",
    "#         tp = cm.iloc[1,1]\n",
    "#         tpr = tp/(tp+fn)\n",
    "#         tnr = tn/(tn+fp)\n",
    "#     else:\n",
    "#         tpr = 0\n",
    "#         tnr = 1\n",
    "#     return tpr, tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tpr(df, _class, _model, threshold=None):    \n",
    "    tpr = metrics.recall_score(df['label_{}'.format(_class)],\n",
    "                               df['{}_class'.format(_model)] == _class)\n",
    "    return tpr\n",
    "\n",
    "def compute_tpr_tnr(df, _class, _model, threshold=None):\n",
    "    \n",
    "    true_col = 'label_{}'.format(_class)\n",
    "    pred_col = '{}_class'.format(_model)\n",
    "    tn = len(df.loc[(df[true_col] == False) & (df[pred_col] != _class)])\n",
    "    fp = len(df.loc[(df[true_col] == False) & (df[pred_col] ==_class)])\n",
    "    fn = len(df.loc[(df[true_col] == True) & (df[pred_col] != _class)])\n",
    "    tp = len(df.loc[(df[true_col] == True) & (df[pred_col] ==_class)])\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        raise ValueError('class {} has no positive - impossible to define tpr'.format(_class))\n",
    "\n",
    "    if tn + fp == 0:\n",
    "        tpr = 1\n",
    "        tnr = 0\n",
    "    else:\n",
    "        tpr = tp/(tp+fn)    \n",
    "        tnr = tn/(tn+fp)\n",
    "    return tpr, tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tpr_by_gender(df, _class, _model, threshold = 0.5):\n",
    "    tpr_m = compute_tpr(df.query('gender == \"M\"'), _class, _model, threshold)\n",
    "    tpr_f = compute_tpr(df.query('gender == \"F\"'), _class, _model, threshold)\n",
    "    return {'M': tpr_m, 'F': tpr_f}\n",
    "\n",
    "def compute_tr_by_gender(df, _class, _model, threshold = 0.5):\n",
    "    tpr_m, tnr_m = compute_tpr_tnr(df.query('gender == \"M\"'), _class, _model, threshold)\n",
    "    tpr_f, tnr_f = compute_tpr_tnr(df.query('gender == \"F\"'), _class, _model, threshold)\n",
    "    return {'TPR_m': tpr_m, 'TPR_f': tpr_f, 'TNR_m': tnr_m, 'TNR_f': tnr_f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _class in CLASS_NAMES:\n",
    "    true_col = 'label_{}'.format(_class)\n",
    "    if len(df.loc[(df[true_col] == True)]) == 0:\n",
    "        continue\n",
    "    for _model in MODEL_NAMES:\n",
    "        tpr_1 = compute_tpr(df, _class, _model)\n",
    "        tpr_2, _ = compute_tpr_tnr(df, _class, _model)\n",
    "        assert tpr_1 == tpr_2, '{} != {}'.format(tpr_1, tpr_2)\n",
    "        #print('{} == {}'.format(tpr_1, tpr_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_df = pd.DataFrame()\n",
    "for _class in frac_female_df.label:\n",
    "    row = {}\n",
    "    row['label'] = _class\n",
    "    for _model, _model_type in MODEL_NAMES.items():\n",
    "        tpr, tnr = compute_tpr_tnr(df, _class, _model)\n",
    "        row['{}_tpr'.format(_model_type)] = tpr\n",
    "        row['{}_tnr'.format(_model_type)] = tnr\n",
    "        gender_trs = compute_tr_by_gender(df, _class, _model)\n",
    "        row['{}_tpr_F'.format(_model_type)] = gender_trs['TPR_f']\n",
    "        row['{}_tpr_M'.format(_model_type)] = gender_trs['TPR_m']\n",
    "        row['{}_tpr_gender_gap'.format(_model_type)] = gender_trs['TPR_f'] - gender_trs['TPR_m']\n",
    "        row['{}_tnr_F'.format(_model_type)] = gender_trs['TNR_f']\n",
    "        row['{}_tnr_M'.format(_model_type)] = gender_trs['TNR_m']\n",
    "        row['{}_tnr_gender_gap'.format(_model_type)] = gender_trs['TNR_f'] - gender_trs['TNR_m']\n",
    "    tpr_df = tpr_df.append(row, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.merge(tpr_df, frac_female_df, on = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_LABELS = [\n",
    "    'accountant', 'acupuncturist', 'architect', 'attorney', 'chiropractor', 'comedian', 'composer', 'dentist',\n",
    "    'dietitian', 'dj', 'filmmaker', 'interior_designer', 'journalist', 'landscape_architect', 'magician',\n",
    "    'massage_therapist', 'model', 'nurse', 'painter', 'paralegal', 'pastor', 'personal_trainer',\n",
    "    'photographer', 'physician', 'poet', 'professor', 'psychologist', 'rapper',\n",
    "    'real_estate_broker', 'software_engineer', 'surgeon', 'teacher', 'yoga_teacher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['label_profession'] = results_df['label'].apply(lambda x: TITLE_LABELS[int(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[['frac_female']+['{}_tpr_gender_gap'.format(_model) for _model in MODEL_NAMES.values()]].corr()[['frac_female']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_gender_gap_cols = ['{}_tpr_gender_gap'.format(_model) for _model in MODEL_NAMES.values()]\n",
    "tnr_gender_gap_cols = ['{}_tnr_gender_gap'.format(_model) for _model in MODEL_NAMES.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_gap_df = results_df[['label_profession', 'frac_female']+tpr_gender_gap_cols+tnr_gender_gap_cols]\n",
    "#gender_gap_df.columns = ['label_profession', 'frac_female']+['{}'.format(_model) for _model in MODEL_NAMES.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_gap_df.sort_values('frac_female', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of comments where new model has lower\n",
    "# TPR gap than the baseline\n",
    "\n",
    "def compute_fraction_improved(df, baseline_model, improved_model):\n",
    "    is_improved = np.abs(df[baseline_model]) >= np.abs(df[improved_model])\n",
    "    return np.mean(is_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for _model in MODEL_NAMES.values():\n",
    "#     print(_model)\n",
    "#     print(compute_fraction_improved(gender_gap_df, 'glove_untuned_tpr_gender_gap', '{}_tpr_gender_gap'.format(_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_cols = ['{}_tpr_gender_gap'.format(_model) for _model in MODEL_NAMES.values()]\n",
    "tnr_cols = ['{}_tnr_gender_gap'.format(_model) for _model in MODEL_NAMES.values()]\n",
    "gender_gap_cols = tpr_cols + tnr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_gap_df[gender_gap_cols].apply(lambda x: np.mean(x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_gap_df[gender_gap_cols].apply(lambda x: np.mean(np.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tpr_gap(df, _model):\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    x = 'frac_female'\n",
    "    y = '{}_tpr_gender_gap'.format(_model)\n",
    "    p1 = sns.regplot(x = x, y = y, data = df)\n",
    "    p1.set(xlabel = \"% Female\", ylabel = \"TPR Gender Gap\", title = _model)\n",
    "\n",
    "    for line in range(0,df.shape[0]):\n",
    "         p1.text(results_df[x][line]+0.01, df[y][line], df['label_profession'][line], horizontalalignment='left', size='medium', color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _model in MODEL_NAMES.values():\n",
    "    if 'untuned' in _model:\n",
    "        plot_tpr_gap(results_df, _model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[['frac_female']+['{}_tpr_gender_gap'.format(_model) for _model in MODEL_NAMES.values()]].corr()[['frac_female']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
