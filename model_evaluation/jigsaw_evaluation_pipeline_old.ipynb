{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YibCLoSLRHp"
   },
   "source": [
    "Copyright 2018 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMykUGMauh9b"
   },
   "source": [
    "# Evaluation code\n",
    "\n",
    "\n",
    "__Disclaimer__\n",
    "*   This notebook contains experimental code, which may be changed without notice.\n",
    "*   The ideas here are some ideas relevant to fairness - they are not the whole story!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook intends to evaluate a list of models on two dimensions:\n",
    "- \"Performance\": How well the model perform to classify the data (intended bias). Currently, we use the AUC.\n",
    "- \"Bias\": How much bias does the model contain (unintended bias). Currently, we use the pinned auc.\n",
    "\n",
    "This script takes the following steps:\n",
    "\n",
    "- Prepare the data:\n",
    "    - a \"performance dataset\" which will be used for the first set of metrics. This dataset is supposed to be similar format to the training data (contain a text and a label).\n",
    "    - a \"bias dataset\" which will be used for the second set of metrics. This data contains a text, a label but also some subgroup information to evaluate the unintended bias on.\n",
    "- Runs predictions: we will convert both datasets to TF-Records and call a batch prediction job on Cloud MLE. The result will be added to our data.\n",
    "- Evaluate metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6mIlNEwOvZHt"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jp8VFI-FZ8Dl"
   },
   "source": [
    "We start by loading some libraries that we will use and customizing the visualization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dBRNXZ06Y5Hw"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q git+https://github.com/conversationai/unintended-ml-bias-analysis@1de676a31de9e43892964f71d1e38e90fc8b331e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0VlJ-_r9VjiV"
   },
   "outputs": [],
   "source": [
    "from unintended_ml_bias import model_bias_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 911,
     "status": "ok",
     "timestamp": 1531335549024,
     "user": {
      "displayName": "Flavien Prost",
      "photoUrl": "//lh5.googleusercontent.com/-2GvWuP8dy24/AAAAAAAAAAI/AAAAAAAAAHI/aCatYKxJMXQ/s50-c-k-no/photo.jpg",
      "userId": "100080410554240838905"
     },
     "user_tz": 240
    },
    "id": "j7-dSMztPn3y",
    "outputId": "3780f546-1e90-4732-c616-13c83140a749"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import getpass\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pkg_resources\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import googleapiclient.discovery as discovery\n",
    "import googleapiclient.errors as errors\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GcwTqGF6z0Cv"
   },
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"red\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GCS_READ_CACHE_MAX_SIZE_MB'] = '0' #Faster to access GCS file + https://github.com/tensorflow/tensorflow/issues/15530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting project config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs.\n",
    "PROJECT_NAME = 'wikidetox'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKa5dXGsvc0d"
   },
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmTTPhT_0IKQ"
   },
   "source": [
    "#### Requirements: \n",
    "\n",
    "Cleaned datasets must be pandas DataFrames and must include the following fields:\n",
    "- `text`: the raw text string of the comment.\n",
    "- `label`: label associated to this comment. Must be True for toxic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYj26OD7jU0B"
   },
   "source": [
    "### Preparing performance set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kcRMuDkjrB34"
   },
   "outputs": [],
   "source": [
    "# User inputs.\n",
    "PERFORMANCE_DATASET = 'gs://kaggle-model-experiments/resources/toxicity_q42017_test.tfrecord'\n",
    "SAMPLE_SIZE_PERFORMANCE = 5000 # Set to None to use all data.\n",
    "\n",
    "TEXT_FEATURE_NAME = 'comment_text'\n",
    "SENTENCE_KEY = 'comment_key'\n",
    "LABEL_NAME = 'frac_neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WeLevR8lvoYo"
   },
   "outputs": [],
   "source": [
    "def load_tf_records_to_pandas(tf_records_path, text_feature_name, label_name, max_n_records=None):\n",
    "  '''Loads tf-records into a pandas dataframe.'''\n",
    "    \n",
    "  if not max_n_records:\n",
    "    max_n_records = float('inf')\n",
    "    \n",
    "  # Read TFRecord file\n",
    "  reader = tf.TFRecordReader()\n",
    "  filename_queue = tf.train.string_input_producer([tf_records_path], num_epochs=1)\n",
    "\n",
    "  _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "  # Define features\n",
    "  read_features = {\n",
    "      text_feature_name: tf.FixedLenFeature([], dtype=tf.string),\n",
    "      label_name: tf.FixedLenFeature([], dtype=tf.float32)\n",
    "  }\n",
    "\n",
    "  # Extract features from serialized data\n",
    "  read_data = tf.parse_single_example(serialized=serialized_example,\n",
    "                                      features=read_features)\n",
    "  \n",
    "  # Read and print data:\n",
    "  sess = tf.InteractiveSession()\n",
    "  \n",
    "  # Many tf.train functions use tf.train.QueueRunner,\n",
    "  # so we need to start it before we read.\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  sess.run(tf.local_variables_initializer())\n",
    "  sess.run(tf.tables_initializer())\n",
    "  tf.train.start_queue_runners(sess)\n",
    "  \n",
    "  d = []\n",
    "  new_line = sess.run(read_data)\n",
    "  count = 0\n",
    "  while new_line:\n",
    "    d.append(new_line)\n",
    "    count += 1\n",
    "    if count >= max_n_records:\n",
    "      break\n",
    "    try:\n",
    "      new_line = sess.run(read_data)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print ('End of file.')\n",
    "      break\n",
    "    if not(count % 100000):\n",
    "      print ('Loaded {} lines.'.format(count))\n",
    "\n",
    "  return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TKLiNrveatqn"
   },
   "outputs": [],
   "source": [
    "test_performance_df = load_tf_records_to_pandas(\n",
    "    PERFORMANCE_DATASET,\n",
    "    TEXT_FEATURE_NAME,\n",
    "    LABEL_NAME,\n",
    "    max_n_records=SAMPLE_SIZE_PERFORMANCE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kx00dXBzwYOn"
   },
   "outputs": [],
   "source": [
    "# Setting the table to match the required format.\n",
    "test_performance_df = test_performance_df.rename(\n",
    "    columns={\n",
    "        TEXT_FEATURE_NAME: 'text',\n",
    "        LABEL_NAME: 'label'\n",
    "    })\n",
    "test_performance_df['label'] = list(map(lambda x :bool(round(x)), list(test_performance_df['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1531336716392,
     "user": {
      "displayName": "Flavien Prost",
      "photoUrl": "//lh5.googleusercontent.com/-2GvWuP8dy24/AAAAAAAAAAI/AAAAAAAAAHI/aCatYKxJMXQ/s50-c-k-no/photo.jpg",
      "userId": "100080410554240838905"
     },
     "user_tz": 240
    },
    "id": "NT_GLyyxvozB",
    "outputId": "2d811842-21aa-4615-af11-bedd8bc7869f"
   },
   "outputs": [],
   "source": [
    "print (len(test_performance_df))\n",
    "test_performance_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1jx1xhsvo63"
   },
   "source": [
    "### Preparing bias set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs.\n",
    "BIAS_SAMPLE_SIZE = 5000 # Set to None to use all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fUhI7en1vFJV"
   },
   "outputs": [],
   "source": [
    "# Loading it from it the unintended_ml_bias github.\n",
    "test_bias_df = pd.read_csv(\n",
    "    pkg_resources.resource_stream(\"unintended_ml_bias\", \"eval_datasets/bias_madlibs_77k.csv\"))\n",
    "test_bias_df['text'] = test_bias_df['Text']\n",
    "test_bias_df['label'] = test_bias_df['Label']\n",
    "test_bias_df['label'] = list(map(lambda x: x=='BAD', test_bias_df['label']))\n",
    "test_bias_df = test_bias_df[['text', 'label']].copy()\n",
    "terms = [line.strip()\n",
    "         for line in pkg_resources.resource_stream(\"unintended_ml_bias\", \"bias_madlibs_data/adjectives_people.txt\")]\n",
    "model_bias_analysis.add_subgroup_columns_from_text(test_bias_df, 'text', terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1531336730684,
     "user": {
      "displayName": "Flavien Prost",
      "photoUrl": "//lh5.googleusercontent.com/-2GvWuP8dy24/AAAAAAAAAAI/AAAAAAAAAHI/aCatYKxJMXQ/s50-c-k-no/photo.jpg",
      "userId": "100080410554240838905"
     },
     "user_tz": 240
    },
    "id": "TUIBwXwjwfmd",
    "outputId": "0471e8bb-c600-4dea-adbf-9f6a7b57245c"
   },
   "outputs": [],
   "source": [
    "if BIAS_SAMPLE_SIZE:\n",
    "    test_bias_df = test_bias_df.sample(n=BIAS_SAMPLE_SIZE, random_state=2018)\n",
    "    test_bias_df = test_bias_df.copy()\n",
    "test_bias_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPcDi-ZLztY1"
   },
   "source": [
    "# Calling model to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model families lists all the models to evaluate.\n",
    "\n",
    "\"Model Families\" allows the results to capture training variance by grouping different training versions of each model together. model_families is a list of lists, each sub-list (\"model_family\") contains the names of different training versions of the same model.\n",
    "\n",
    "##### Format.\n",
    "MODEL_FAMILIES lists all the subfamilies. One subfamily is a list of models with the pattern (\\$MODEL_NAME((:\\$VERSION_NAME)?)).\n",
    "If the version is not specified, the default one is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs.\n",
    "MODEL_FAMILIES = [\n",
    "    ['tf_gru_attention:v_20180914_163804']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_XupbZxhcik"
   },
   "source": [
    "#### Converting dataframe to TF-Records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "77F5vjFaCLgJ"
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _write_pandas_to_tf_records(df, gcs_path):\n",
    "  '''Write a pandas `DataFrame` to a tf_record.\n",
    "  \n",
    "  Args:\n",
    "    df: pandas `DataFrame`. It must include the fields 'sentence'.\n",
    "    gcs_path: where to write the tf records.\n",
    "  \n",
    "  Note: TFRecords will have fields `sentence` and `key`.\n",
    "  '''\n",
    "  \n",
    "  writer = tf.python_io.TFRecordWriter(gcs_path)\n",
    "  for i in range(len(df)):\n",
    "    \n",
    "      if not i % 10000:\n",
    "          print ('Preparing train data: {}/{}'.format(i, len(df)))\n",
    "      \n",
    "      # Create a feature\n",
    "      feature = {TEXT_FEATURE_NAME: _bytes_feature(tf.compat.as_bytes(df['text'].iloc[i])),\n",
    "                 SENTENCE_KEY: _int64_feature(i)}\n",
    "      example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "      # Serialize to string and write on the file\n",
    "      writer.write(example.SerializeToString())\n",
    "\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBpmc3_IhmIb"
   },
   "source": [
    "#### Running batch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vXQgIwJJL6wS"
   },
   "outputs": [],
   "source": [
    "def _make_batch_job_body(project_name, input_paths, output_path,\n",
    "        model_name, region='us-central1', data_format='TF_RECORD',\n",
    "        version_name=None, max_worker_count=None,\n",
    "        runtime_version=None):\n",
    "  '''Creates the request body for Cloud MLE batch prediction job.'''\n",
    "\n",
    "  project_id = 'projects/{}'.format(project_name)\n",
    "  model_id = '{}/models/{}'.format(project_id, model_name)\n",
    "  if version_name:\n",
    "    version_id = '{}/versions/{}'.format(model_id, version_name)\n",
    "\n",
    "  # Make a jobName of the format \"model_name_batch_predict_YYYYMMDD_HHMMSS\"\n",
    "  timestamp = time.strftime('%Y%m%d_%H%M%S', time.gmtime())\n",
    "\n",
    "  # Make sure the project name is formatted correctly to work as the basis\n",
    "  # of a valid job name.\n",
    "  clean_project_name = re.sub(r'\\W+', '_', project_name)\n",
    "\n",
    "  job_id = '{}_{}_{}'.format(clean_project_name, model_name,\n",
    "                             timestamp)\n",
    "\n",
    "  # Start building the request dictionary with required information.\n",
    "  body = {'jobId': job_id,\n",
    "          'predictionInput': {\n",
    "              'dataFormat': data_format,\n",
    "              'inputPaths': input_paths,\n",
    "              'outputPath': output_path,\n",
    "              'region': region\n",
    "          }}\n",
    "\n",
    "  # Use the version if present, the model (its default version) if not.\n",
    "  if version_name:\n",
    "    body['predictionInput']['versionName'] = version_id\n",
    "  else:\n",
    "    body['predictionInput']['modelName'] = model_id\n",
    "\n",
    "  # Only include a maximum number of workers or a runtime version if specified.\n",
    "  # Otherwise let the service use its defaults.\n",
    "  if max_worker_count:\n",
    "    body['predictionInput']['maxWorkerCount'] = max_worker_count\n",
    "\n",
    "  if runtime_version:\n",
    "    body['predictionInput']['runtimeVersion'] = runtime_version\n",
    "\n",
    "  return body\n",
    "\n",
    "\n",
    "def _call_batch_job(project_name, input_paths, output_path, model_name, version_name=None):\n",
    "  '''Calls a batch prediction job on Cloud MLE.'''\n",
    "  \n",
    "  batch_predict_body = _make_batch_job_body(\n",
    "      project_name, input_paths, output_path, model_name, version_name=version_name)\n",
    "\n",
    "  project_id = 'projects/{}'.format(project_name)\n",
    "\n",
    "  ml = discovery.build('ml', 'v1')\n",
    "  request = ml.projects().jobs().create(parent=project_id,\n",
    "                                        body=batch_predict_body)\n",
    "\n",
    "  try:\n",
    "    response = request.execute()\n",
    "    print('state : {}'.format(response['state']))\n",
    "    return response['jobId']\n",
    "\n",
    "  except errors.HttpError as err:\n",
    "    # Something went wrong, print out some information.\n",
    "    print('There was an error getting the prediction results.' +\n",
    "          'Check the details:')\n",
    "    print(err._get_reason())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmIa3wRZhu-1"
   },
   "source": [
    "#### Map predictions results to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VDGt_dWgjCnB"
   },
   "outputs": [],
   "source": [
    "def _check_job_over(project_name, job_name):\n",
    "  '''Sleeps until the batch job is over.'''\n",
    "  \n",
    "  project_id = 'projects/{}'.format(project_name)\n",
    "  clean_project_name = re.sub(r'\\W+', '_', project_name)\n",
    "  \n",
    "  ml = discovery.build('ml', 'v1')\n",
    "  request = ml.projects().jobs().get(name='projects/{}/jobs/{}'.format(clean_project_name, job_name))\n",
    "  \n",
    "  job_completed = False\n",
    "  k = 0\n",
    "  while not job_completed:\n",
    "    k += 1\n",
    "    response = request.execute()\n",
    "    job_completed = (response['state'] == 'SUCCEEDED')\n",
    "    if not (k % 5) and not job_completed:\n",
    "      print ('Waiting for prediction job to complete. Min elapsed: {}'.format(0.5*k))\n",
    "      time.sleep(30)\n",
    "  \n",
    "  print ('Prediction job completed.')\n",
    "\n",
    "    \n",
    "def _combine_prediction_to_df(df, prediction_file, model_col_name):\n",
    "  '''Loads the prediction files and adds them to the DataFrame.'''\n",
    "  \n",
    "  def load_predictions(prediction_file):\n",
    "    with file_io.FileIO(prediction_file, 'r') as _file:\n",
    "      # prediction file needs to fit in memory.\n",
    "      predictions = [json.loads(line) for line in _file] \n",
    "    return predictions\n",
    "  \n",
    "  predictions = load_predictions(prediction_file)\n",
    "  predictions = sorted(predictions, key = lambda x: x[SENTENCE_KEY])\n",
    "  \n",
    "  if len(predictions) != len(df):\n",
    "    raise ValueError('The dataframe and the prediction file do not contain the same number of lines.')\n",
    "  \n",
    "  prediction_proba = [x[LABEL_NAME][0] for x in predictions]\n",
    "  \n",
    "  df[model_col_name] = prediction_proba\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XT3qsSFziNPL"
   },
   "source": [
    "#### Combine everything in one single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Vt5YhVG_iFuA"
   },
   "outputs": [],
   "source": [
    "# High level function that should be used by user.\n",
    "\n",
    "def call_model_predictions_from_df(df, project_name, tmp_tfrecords_gcs_path, \n",
    "                                   tmp_tfrecords_with_predictions_gcs_path, model_name, \n",
    "                                   version_name=None, rewrite=True):\n",
    "  '''Calls a prediction job.\n",
    "  \n",
    "  Args:\n",
    "    - df: a pandas `DataFrame`. Must contain a `text` field.\n",
    "    - project_name: gcp project name.\n",
    "    - tmp_tfrecords_gcs_path: gcs path to store tf_records, which will be inputs\n",
    "        to batch prediction job.\n",
    "    - tmp_tfrecords_with_predictions_gcs_path: gcs path to store tf_records, \n",
    "        which will be outputs to batch prediction job.\n",
    "    - model_name: Model name used to run predictions.\n",
    "        The model must take as inputs TF-Records with fields $TEXT_FEATURE_NAME\n",
    "        and $SENTENCE_KEY, and should return a dictionary including the field $LABEL_NAME.\n",
    "    - version_name: Model version to run predictions.\n",
    "        If None, it will use default version of the model.\n",
    "    - rewrite: whether to rewrite the tmp_tfrecords_gcs_path.\n",
    "        If False, it will check if file is already existing and will potentially\n",
    "        use pre-existing file to call predictions, without re-running preprocessing.\n",
    "\n",
    "  Returns:\n",
    "    - job_id: the job_id of the prediction job.\n",
    "  '''\n",
    "  \n",
    "  # Create tf-records if necessary.\n",
    "  if rewrite or not file_io.file_exists(tmp_tfrecords_gcs_path):\n",
    "    _write_pandas_to_tf_records(df, tmp_tfrecords_gcs_path)\n",
    "  \n",
    "  # Call batch prediction job. \n",
    "  job_id = _call_batch_job(\n",
    "    project_name,\n",
    "    input_paths=tmp_tfrecords_gcs_path,\n",
    "    output_path=tmp_tfrecords_with_predictions_gcs_path,\n",
    "    model_name=model_name,\n",
    "    version_name=version_name)\n",
    "  \n",
    "  return job_id\n",
    "\n",
    "\n",
    "def add_model_predictions_to_df(job_id, df, project_name,\n",
    "                                tmp_tfrecords_with_predictions_gcs_path, column_name_of_model):\n",
    "  '''Adds prediction results to the pandas dataframe.\n",
    "  \n",
    "  Args:\n",
    "    - job_id: the job_id of the prediction job.\n",
    "    - df: a pandas `DataFrame`. Must contain a `text` field.\n",
    "    - project_name: gcp project name.\n",
    "    - tmp_tfrecords_with_predictions_gcs_path: gcs path to store tf_records, \n",
    "        which will be outputs to batch prediction job.\n",
    "    - column_name_of_model: Name of the added column.\n",
    "  \n",
    "  Returns:\n",
    "    - df: a pandas ` DataFrame` with an added column named 'column_name_of_model'\n",
    "        containing the prediction values.\n",
    "  '''\n",
    "\n",
    "  # Waits for batch job to be over.\n",
    "  _check_job_over(project_name, job_id)\n",
    "  \n",
    "  # Add one prediction column to the database.\n",
    "  tf_records_path = os.path.join(tmp_tfrecords_with_predictions_gcs_path, 'prediction.results-00000-of-00001')\n",
    "  df_with_predictions = _combine_prediction_to_df(df, tf_records_path, column_name_of_model)\n",
    "  \n",
    "  return df_with_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kBhrAro_20ih"
   },
   "source": [
    "### Running entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs.\n",
    "GCS_BUCKET='gs://kaggle-model-experiments/'\n",
    "\n",
    "TF_RECORD_PERFORMANCE_INPUT = os.path.join(\n",
    "    GCS_BUCKET,\n",
    "    getpass.getuser(),\n",
    "    'tfrecords/test_performance.tfrecords')\n",
    "TF_RECORD_PERFORMANCE_PREDICTION_PREFIX = os.path.join(\n",
    "    GCS_BUCKET,\n",
    "    getpass.getuser(),\n",
    "    'tfrecords/test_performance_with_predictions_') \n",
    "\n",
    "TF_RECORD_BIAS_INPUT = os.path.join(\n",
    "    GCS_BUCKET,\n",
    "    getpass.getuser(),\n",
    "    'test_bias.tfrecords'\n",
    ")\n",
    "TF_RECORD_BIAS_PREDICTION_PREFIX = os.path.join(\n",
    "    GCS_BUCKET,\n",
    "    getpass.getuser(),\n",
    "    'test_bias_with_predictions_'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Running predictions\n",
    "job_ids_performance = []\n",
    "job_ids_bias = []\n",
    "for subfamily in MODEL_FAMILIES:\n",
    "  for model_full_name in subfamily:\n",
    "        \n",
    "    model_full_name_split = model_full_name.split(':')\n",
    "    model = model_full_name_split[0]\n",
    "    if len(model_full_name_split) > 1:\n",
    "      version = model_full_name_split[1]\n",
    "    else:\n",
    "      version = None\n",
    "        \n",
    "    job_id = call_model_predictions_from_df(\n",
    "        test_performance_df,\n",
    "        project_name=PROJECT_NAME,\n",
    "        tmp_tfrecords_gcs_path=TF_RECORD_PERFORMANCE_INPUT,\n",
    "        tmp_tfrecords_with_predictions_gcs_path=TF_RECORD_PERFORMANCE_PREDICTION_PREFIX + model_full_name,\n",
    "        model_name=model,\n",
    "        version_name=version\n",
    "    )\n",
    "    job_ids_performance.append(job_id)\n",
    "        \n",
    "    job_id = call_model_predictions_from_df(\n",
    "        test_bias_df,\n",
    "        project_name=PROJECT_NAME,\n",
    "        tmp_tfrecords_gcs_path=TF_RECORD_BIAS_INPUT,\n",
    "        tmp_tfrecords_with_predictions_gcs_path=TF_RECORD_BIAS_PREDICTION_PREFIX+ model_full_name,\n",
    "        model_name=model,\n",
    "        version_name=version\n",
    "    )\n",
    "    job_ids_bias.append(job_id)\n",
    "\n",
    "\n",
    "# Collecting predictions        \n",
    "i = 0 \n",
    "for subfamily in MODEL_FAMILIES:\n",
    "  for model_full_name in subfamily:\n",
    "    \n",
    "    job_id = job_ids_performance[i]\n",
    "    test_performance_df = add_model_predictions_to_df(\n",
    "        job_id,\n",
    "        test_performance_df,\n",
    "        project_name=PROJECT_NAME,\n",
    "        tmp_tfrecords_with_predictions_gcs_path=TF_RECORD_BIAS_PREDICTION_PREFIX + model_full_name,\n",
    "        column_name_of_model=model_full_name\n",
    "    )\n",
    "        \n",
    "    job_id = job_ids_bias[i]\n",
    "    test_bias_df = add_model_predictions_to_df(\n",
    "        job_id,\n",
    "        test_bias_df,\n",
    "        project_name=PROJECT_NAME,\n",
    "        tmp_tfrecords_with_predictions_gcs_path=TF_RECORD_BIAS_PREDICTION_PREFIX + model_full_name,\n",
    "        column_name_of_model=model_full_name\n",
    "    )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1530641283264,
     "user": {
      "displayName": "Flavien Prost",
      "photoUrl": "//lh5.googleusercontent.com/-2GvWuP8dy24/AAAAAAAAAAI/AAAAAAAAAHI/aCatYKxJMXQ/s50-c-k-no/photo.jpg",
      "userId": "100080410554240838905"
     },
     "user_tz": 240
    },
    "id": "Y7R4heIB5GaV",
    "outputId": "e8e0c3bc-96d8-4635-865a-275052054df8"
   },
   "outputs": [],
   "source": [
    "test_performance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1530641286091,
     "user": {
      "displayName": "Flavien Prost",
      "photoUrl": "//lh5.googleusercontent.com/-2GvWuP8dy24/AAAAAAAAAAI/AAAAAAAAAHI/aCatYKxJMXQ/s50-c-k-no/photo.jpg",
      "userId": "100080410554240838905"
     },
     "user_tz": 240
    },
    "id": "Ln2BXOg4Q6GP",
    "outputId": "bb5288e8-9f10-4796-b36e-42f5c02cb148"
   },
   "outputs": [],
   "source": [
    "test_bias_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8m8QI4qEjtcY"
   },
   "source": [
    "# Run evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhwSHsMtO9fF"
   },
   "source": [
    "## Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our performance data is in DataFrame df, with columns:\n",
    "\n",
    "text: Full text of the comment.\n",
    "label: True if the comment is Toxic, False otherwise.\n",
    "< model name >: One column per model, cells contain the score from that model.\n",
    "You can run the analysis below on any data in this format. Subgroup labels can be generated via words in the text as done above, or come from human labels if you have them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XUZYCq-6N8MK"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1530641399913,
     "user": {
      "displayName": "Flavien Prost",
      "photoUrl": "//lh5.googleusercontent.com/-2GvWuP8dy24/AAAAAAAAAAI/AAAAAAAAAHI/aCatYKxJMXQ/s50-c-k-no/photo.jpg",
      "userId": "100080410554240838905"
     },
     "user_tz": 240
    },
    "id": "yc8SWZbqMwA4",
    "outputId": "6e9399b8-ce22-42bb-c318-959bae73f6c0"
   },
   "outputs": [],
   "source": [
    "for model_family in MODEL_FAMILIES:\n",
    "  auc_list = []\n",
    "  for model in model_family:\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_performance_df['label'], test_performance_df[model])\n",
    "    auc_list.append(metrics.auc(fpr, tpr))\n",
    "  print ('Auc for model {}: {}'.format(model, np.mean(auc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTrKsfIcxoBh"
   },
   "source": [
    "## Unintended Bias Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3ZJSKY8FHFH"
   },
   "source": [
    "### Data Format\n",
    "At this point, our bias data is in DataFrame df, with columns:\n",
    "\n",
    "*   text: Full text of the comment.\n",
    "*   label: True if the comment is Toxic, False otherwise.\n",
    "*   < model name >: One column per model, cells contain the score from that model.\n",
    "*   < subgroup >: One column per identity, True if the comment mentions this identity.\n",
    "\n",
    "You can run the analysis below on any data in this format. Subgroup labels can be \n",
    "generated via words in the text as done above, or come from human labels if you have them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPXk78apx2-Z"
   },
   "source": [
    "### Pinned AUC\n",
    "Pinned AUC measures the extent of unintended bias of a real-value score function\n",
    "by measuring each sub-group's divergence from the general distribution.\n",
    "\n",
    "Let $D$ represent the full data set and $D_g$ be the set of examples in subgroup\n",
    "$g$. Then:\n",
    "\n",
    "\n",
    "$$ Pinned \\ dataset \\ for \\ group \\ g = pD_g = s(D_g) + s(D), |s(D_g)| = |s(D)| $$\n",
    "\n",
    "$$ Pinned \\ AUC \\ for \\ group \\ g = pAUC_g = AUC(pD_g) $$\n",
    "\n",
    "$$ Pinned \\ AUC \\ Squared \\ Equality \\ Difference = \\Sigma_{g \\in G}(AUC - pAUC_g)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65U3PzABPiE6"
   },
   "source": [
    "### Pinned AUC Equality Difference\n",
    "The table below shows the pinned AUC equality difference for each model family.\n",
    "Lower scores (lighter red) represent more similarity between each group's pinned AUC, which means\n",
    "less unintended bias.\n",
    "\n",
    "On this set, the wiki_debias_cnn model demonstrates least unintended bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1019,
     "status": "error",
     "timestamp": 1530641407221,
     "user": {
      "displayName": "Flavien Prost",
      "photoUrl": "//lh5.googleusercontent.com/-2GvWuP8dy24/AAAAAAAAAAI/AAAAAAAAAHI/aCatYKxJMXQ/s50-c-k-no/photo.jpg",
      "userId": "100080410554240838905"
     },
     "user_tz": 240
    },
    "id": "W8p5iHW2RZmN",
    "outputId": "e4b3f09b-ad81-48dc-b9ad-ef2a77233044"
   },
   "outputs": [],
   "source": [
    "eq_diff = model_bias_analysis.per_subgroup_auc_diff_from_overall(\n",
    "    test_bias_df, terms, MODEL_FAMILIES, squared_error=True, normed_auc=True)\n",
    "# sort to guarantee deterministic output\n",
    "eq_diff.sort_values(by=['model_family'], inplace=True)\n",
    "eq_diff.reset_index(drop=True, inplace=True)\n",
    "eq_diff.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7bEC5cAsyC05"
   },
   "source": [
    "### Pinned AUC Graphs\n",
    "The graphs below show per-group Pinned AUC for each subgroup and each model. Each\n",
    "identity group shows 3 points, each representing the pinned AUC for one training \n",
    "version of the model. More consistency among the values represents less unintended bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1552,
     "status": "ok",
     "timestamp": 1530303491427,
     "user": {
      "displayName": "Flavien Prost",
      "photoUrl": "//lh5.googleusercontent.com/-2GvWuP8dy24/AAAAAAAAAAI/AAAAAAAAAHI/aCatYKxJMXQ/s50-c-k-no/photo.jpg",
      "userId": "100080410554240838905"
     },
     "user_tz": 240
    },
    "id": "QqMmPTreOCPf",
    "outputId": "39cc5e70-b76d-413e-b286-e25f7a68e732"
   },
   "outputs": [],
   "source": [
    "pinned_auc_results = model_bias_analysis.per_subgroup_aucs(test_bias_df, terms, MODEL_FAMILIES, 'label')\n",
    "for family in MODEL_FAMILIES:\n",
    "  name = model_bias_analysis.model_family_name(family)\n",
    "  model_bias_analysis.per_subgroup_scatterplots(\n",
    "      pinned_auc_results,\n",
    "      'subgroup',\n",
    "      name + '_aucs',\n",
    "      name + ' Pinned AUC',\n",
    "      y_lim=(0., 1.0))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "jigsaw-evaluation-pipeline.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
